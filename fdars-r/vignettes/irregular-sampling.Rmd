---
title: "Working with Irregular Functional Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Working with Irregular Functional Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  out.width = "100%"
)
```

```{r setup}
library(fdars)
library(ggplot2)
theme_set(theme_minimal())
```

## Introduction

Many real-world functional data are irregularly sampled, meaning:

- Different curves are observed at different time points
- Observations may be sparse (few points per curve)
- Sampling density may vary within or across curves

The `fdars` package provides the `irregFdata` class to handle such data naturally, avoiding the need for imputation or interpolation before analysis.

### Common Applications

- **Longitudinal studies**: Patients visited at different times
- **Sensor data**: Measurements with gaps or varying frequency
- **Environmental monitoring**: Non-uniform temporal sampling
- **Financial data**: Trades occurring at irregular intervals

## The irregFdata Class

### Creating irregFdata Objects

An `irregFdata` object stores observation times and values as lists:

```{r create-irreg}
# Three curves with different observation points
argvals <- list(
  c(0.0, 0.3, 0.7, 1.0),       # 4 points
  c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0),  # 6 points
  c(0.1, 0.5, 0.9)              # 3 points
)

X <- list(
  c(0.1, 0.5, 0.3, 0.2),
  c(0.0, 0.4, 0.8, 0.6, 0.4, 0.1),
  c(0.3, 0.7, 0.2)
)

ifd <- irregFdata(argvals, X)
print(ifd)
```

### Structure

```{r structure}
# Number of observations
ifd$n

# Observation counts per curve
sapply(ifd$X, length)

# Domain range
ifd$rangeval
```

### Checking Data Type

```{r is-irregular}
# Regular fdata
fd_regular <- fdata(matrix(rnorm(100), 10, 10))
is.irregular(fd_regular)

# Irregular fdata
is.irregular(ifd)
```

## Sparsifying Regular Data

Use `sparsify()` to convert regular `fdata` to `irregFdata`:

```{r sparsify}
# Start with regular data
t <- seq(0, 1, length.out = 100)
fd <- simFunData(n = 10, argvals = t, M = 5, seed = 42)

# Create sparse version with 15-30 observations per curve
ifd <- sparsify(fd, minObs = 15, maxObs = 30, seed = 123)
print(ifd)
```

### Visualizing Sparse Data

```{r plot-sparse}
autoplot(ifd) +
  labs(title = "Sparsified Functional Data")
```

### Non-Uniform Sparsification

Control sampling density with a probability function:

```{r nonuniform-sparsify, fig.height = 8, fig.width = 10}
# More observations in the middle
prob_middle <- function(t) dnorm(t, mean = 0.5, sd = 0.2)

# More observations at the edges (U-shaped probability)
prob_edges <- function(t) 0.1 + 4 * (t - 0.5)^2

fd <- simFunData(n = 20, argvals = t, M = 5, seed = 42)

ifd_uniform <- sparsify(fd, minObs = 15, maxObs = 25, seed = 123)
ifd_middle <- sparsify(fd, minObs = 15, maxObs = 25, prob = prob_middle, seed = 123)
ifd_edges <- sparsify(fd, minObs = 15, maxObs = 25, prob = prob_edges, seed = 123)

p1 <- autoplot(ifd_uniform) + labs(title = "Uniform Sampling")
p2 <- autoplot(ifd_middle) + labs(title = "Dense in Middle")
p3 <- autoplot(ifd_edges) + labs(title = "Dense at Edges")
gridExtra::grid.arrange(p1, p2, p3, ncol = 1)
```

## Converting to Regular Grid

Use `as.fdata()` to convert back to regular `fdata`:

### Method: NA Fill

Only exact matches are filled; other points get `NA`:

```{r as-fdata-na}
ifd <- sparsify(fd[1:3], minObs = 10, maxObs = 20, seed = 123)

fd_na <- as.fdata(ifd, method = "na")

# Show some data with NAs
fd_na$data[1, 1:20]
```

### Method: Linear Interpolation

Interpolate between observed points:

```{r as-fdata-linear}
# Specify target grid
target_grid <- seq(0, 1, length.out = 50)
fd_interp <- as.fdata(ifd, argvals = target_grid, method = "linear")

# No NAs within observed range
sum(is.na(fd_interp$data))
```

```{r compare-methods, fig.height = 4, fig.width = 10}
p1 <- autoplot(ifd[1]) + labs(title = "Original Sparse Data (1 curve)")
p2 <- autoplot(fd_interp[1]) + labs(title = "After Linear Interpolation")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

## Basis Representation for Sparse Data

Once irregular data is converted to a regular grid, basis representation provides a powerful way to:

- **Smooth** noisy observations
- **Reduce dimensionality** (from many grid points to few coefficients)
- **Regularize** the curves for downstream analysis (FPCA, regression, clustering)

For comprehensive coverage of basis functions, see `vignette("basis-representation")`.

### B-spline Projection

Project curves onto a B-spline basis for smoothing and dimensionality reduction:

```{r basis-projection, fig.height = 4, fig.width = 10}
# Simulate functional data
set.seed(123)
fd_sim <- simFunData(n = 15, argvals = seq(0, 1, length.out = 100), M = 5, seed = 42)

# Add noise to simulate measurement error
fd_noisy <- fd_sim
fd_noisy$data <- fd_noisy$data + matrix(rnorm(length(fd_noisy$data), sd = 0.3),
                                         nrow = nrow(fd_noisy$data))

# Project onto B-spline basis (smooths the curves)
coefs <- fdata2basis(fd_noisy, nbasis = 12, type = "bspline")
fd_basis <- basis2fdata(coefs, argvals = fd_noisy$argvals)

# Compare using ggplot2
p1 <- autoplot(fd_noisy, alpha = 0.7) + labs(title = "Noisy Data")
p2 <- autoplot(fd_basis, alpha = 0.7) + labs(title = "B-spline Smoothed (12 basis)")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

### P-spline Smoothing

P-splines with automatic smoothing parameter selection provide robust results for noisy data:

```{r pspline-smoothing, fig.height = 4, fig.width = 10}
# Create noisy data for P-spline demo
set.seed(456)
fd_for_pspline <- simFunData(n = 15, argvals = seq(0, 1, length.out = 100), M = 5, seed = 42)
fd_for_pspline$data <- fd_for_pspline$data + matrix(rnorm(length(fd_for_pspline$data), sd = 0.3),
                                                      nrow = nrow(fd_for_pspline$data))

# P-spline with fixed lambda (automatic selection can be unstable for some data)
pspline_result <- pspline(fd_for_pspline, nbasis = 20, lambda = 0.01)

# Compare (pspline returns a list; extract $fdata for the smoothed curves)
p1 <- autoplot(fd_for_pspline, alpha = 0.7) + labs(title = "Noisy Data")
p2 <- autoplot(pspline_result$fdata, alpha = 0.7) + labs(title = "P-spline Smoothed")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

### Choosing the Right Approach

| Approach | Best For | Key Parameters |
|----------|----------|----------------|
| B-spline projection | Clean data, fast computation | `nbasis`, `type` |
| P-spline smoothing | Noisy data, automatic smoothing | `nbasis`, `lambda` |
| Fourier basis | Periodic/seasonal patterns | `nbasis`, `type="fourier"` |

**Tip**: Use `fdata2basis.cv()` to automatically select the optimal number of basis functions via cross-validation.

## Operations on Irregular Data

### Integration

Compute integrals using trapezoidal rule:

```{r integration}
# Create data with known integral
argvals <- list(
  seq(0, 1, length.out = 50),
  seq(0, 1, length.out = 30)
)
# Constant function = 1 should integrate to 1
X <- list(rep(1, 50), rep(2, 30))
ifd <- irregFdata(argvals, X)

integrals <- int.simpson.irregFdata(ifd)
print(integrals)  # Should be approximately 1 and 2
```

### Lp Norms

```{r norms}
# L2 norm of constant function c is c
norms <- norm.irregFdata(ifd, p = 2)
print(norms)
```

### Mean Function Estimation

Estimate the mean using kernel smoothing:

```{r mean-estimation}
# Simulate many curves
fd <- simFunData(n = 50, argvals = t, M = 5, seed = 42)
ifd <- sparsify(fd, minObs = 15, maxObs = 30, seed = 123)

# Estimate mean
mean_fd <- mean(ifd, bandwidth = 0.1)
autoplot(mean_fd) + labs(title = "Estimated Mean Function")
```

```{r compare-mean}
# Compare to true sample mean (from original data)
true_mean <- colMeans(fd$data)

# Create comparison data frame
compare_df <- rbind(
  data.frame(t = t, value = true_mean, type = "True Sample Mean"),
  data.frame(t = mean_fd$argvals, value = mean_fd$data[1,], type = "Kernel Estimate")
)

ggplot(compare_df, aes(x = t, y = value, color = type)) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c("True Sample Mean" = "blue", "Kernel Estimate" = "red")) +
  labs(title = "Mean Comparison", x = "t", y = "Mean", color = NULL) +
  theme_minimal()
```

### Distance Matrix

Compute pairwise Lp distances:

```{r distances}
ifd <- sparsify(fd[1:5], minObs = 20, maxObs = 40, seed = 123)
D <- metric.lp.irregFdata(ifd, p = 2)
round(D, 2)
```

## Subsetting

Extract specific observations:

```{r subsetting}
ifd <- sparsify(fd[1:10], minObs = 10, maxObs = 20, seed = 123)

# Single observation
ifd_sub1 <- ifd[1]
print(ifd_sub1)

# Multiple observations
ifd_sub23 <- ifd[2:3]
print(ifd_sub23)

# Negative indexing
ifd_not1 <- ifd[-1]
ifd_not1$n
```

## Metadata Support

Store additional covariates with the data:

```{r metadata}
argvals <- list(c(0, 0.5, 1), c(0, 1), c(0, 0.3, 0.7, 1))
X <- list(c(1, 2, 1), c(0, 2), c(1, 1.5, 1.5, 1))
meta <- data.frame(
  group = c("treatment", "control", "treatment"),
  age = c(45, 52, 38)
)

ifd <- irregFdata(argvals, X,
                  id = c("patient_001", "patient_002", "patient_003"),
                  metadata = meta)

# Access metadata
ifd$id
ifd$metadata

# Subsetting preserves metadata
ifd[1]$metadata
```

## Real-World Use Cases

### Case 1: Sparse Longitudinal Data

```{r longitudinal, fig.height = 4}
set.seed(42)

# Simulate patient growth curves with irregular visits
n_patients <- 15

argvals_list <- lapply(1:n_patients, function(i) {
  # Random number of visits (5-12)
  n_visits <- sample(5:12, 1)
  # Random visit times in [0, 2] years
  sort(runif(n_visits, 0, 2))
})

# Growth model: baseline + linear growth + random effect
X_list <- lapply(1:n_patients, function(i) {
  baseline <- rnorm(1, 50, 5)
  growth_rate <- rnorm(1, 10, 2)
  noise <- rnorm(length(argvals_list[[i]]), 0, 1)
  baseline + growth_rate * argvals_list[[i]] + noise
})

ifd_growth <- irregFdata(argvals_list, X_list,
                          names = list(main = "Growth Curves",
                                       xlab = "Age (years)",
                                       ylab = "Height (cm)"))

# Estimate mean growth curve
mean_growth <- mean(ifd_growth,
                                argvals = seq(0, 2, length.out = 50),
                                bandwidth = 0.2)

# Visualize with ggplot2
mean_df <- data.frame(t = mean_growth$argvals, mean = mean_growth$data[1,])
autoplot(ifd_growth, alpha = 0.8) +
  geom_line(data = mean_df, aes(x = t, y = mean), color = "red", linewidth = 1.5, inherit.aes = FALSE) +
  labs(title = "Growth Curves with Estimated Mean")
```

### Case 2: Sensor Data with Gaps

```{r sensor-gaps, fig.height = 4}
set.seed(123)

# Simulate sensor data with random gaps
t_full <- seq(0, 24, by = 0.1)  # 24 hours

simulate_sensor <- function() {
  # Random gaps (sensor offline)
  n_gaps <- sample(3:8, 1)
  gap_starts <- sort(runif(n_gaps, 0, 23))
  gap_lengths <- runif(n_gaps, 0.5, 2)

  keep <- rep(TRUE, length(t_full))
  for (i in 1:n_gaps) {
    keep[t_full >= gap_starts[i] & t_full < gap_starts[i] + gap_lengths[i]] <- FALSE
  }

  t_obs <- t_full[keep]
  # Temperature with daily cycle
  temp <- 20 + 5 * sin(2 * pi * t_obs / 24) + rnorm(length(t_obs), 0, 0.5)

  list(t = t_obs, x = temp)
}

sensors <- lapply(1:5, function(i) simulate_sensor())

ifd_sensor <- irregFdata(
  argvals = lapply(sensors, `[[`, "t"),
  X = lapply(sensors, `[[`, "x"),
  names = list(main = "Temperature Sensors",
               xlab = "Hour of Day",
               ylab = "Temperature (C)")
)

autoplot(ifd_sensor, alpha = 0.8)
```

## Best Practices

### When to Use Irregular Representation

| Scenario | Recommendation |
|----------|----------------|
| Few missing points | Use regular `fdata` with NA |
| Systematic sparsity | Use `irregFdata` |
| Very dense data | Use regular `fdata` |
| Mixed observation times | Use `irregFdata` |

### Memory Considerations

`irregFdata` is more memory-efficient when data is sparse:

```{r memory}
# Regular: always stores n x m values
n <- 100
m <- 1000
regular_size <- n * m * 8  # bytes (double)

# Irregular: stores only observed values
avg_obs <- 50  # average observations per curve
irreg_size <- n * avg_obs * 8 * 2  # values + argvals

cat("Regular (n=100, m=1000):", regular_size / 1024, "KB\n")
cat("Irregular (n=100, ~50 obs each):", irreg_size / 1024, "KB\n")
```

### Preprocessing Recommendations

1. **Quality control**: Remove curves with too few observations
2. **Domain alignment**: Ensure all curves span similar ranges
3. **Outlier removal**: Check for obviously erroneous values
4. **Bandwidth selection**: Use cross-validation for kernel smoothing

```{r quality-control}
# Remove curves with fewer than 5 observations
min_obs <- 5
keep <- sapply(ifd$X, length) >= min_obs

if (any(!keep)) {
  ifd_clean <- ifd[keep]
  cat("Removed", sum(!keep), "curves with <", min_obs, "observations\n")
}
```

## Summary

| Function | Purpose |
|----------|---------|
| `irregFdata()` | Create irregular functional data objects |
| `is.irregular()` | Check if object is irregFdata |
| `sparsify()` | Convert regular to irregular data |
| `as.fdata()` | Convert irregular to regular (with interpolation) |
| `int.simpson.irregFdata()` | Compute integrals |
| `norm.irregFdata()` | Compute Lp norms |
| `mean.irregFdata()` | Estimate mean via kernel smoothing |
| `metric.lp.irregFdata()` | Compute pairwise distances |
