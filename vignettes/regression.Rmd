---
title: "Functional Regression"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Functional Regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Introduction

Functional regression predicts a scalar response from functional predictors.
The general model is:
$$Y_i = \alpha + \int \beta(t) X_i(t) dt + \epsilon_i$$

where $X_i(t)$ is a functional predictor and $\beta(t)$ is a functional
coefficient.

**fdars** provides three main approaches:

1. **Principal Component Regression** (`fregre.pc`)
2. **Basis Expansion Regression** (`fregre.basis`)
3. **Nonparametric Regression** (`fregre.np`)

```{r setup}
library(fdars)

# Generate example data
set.seed(42)
n <- 100
m <- 50
t_grid <- seq(0, 1, length.out = m)

# Functional predictors
X <- matrix(0, n, m)
for (i in 1:n) {
  X[i, ] <- sin(2 * pi * t_grid) * rnorm(1, 1, 0.3) +
            cos(4 * pi * t_grid) * rnorm(1, 0, 0.2) +
            rnorm(m, sd = 0.1)
}

fd <- fdata(X, argvals = t_grid)

# True coefficient function
beta_true <- sin(2 * pi * t_grid)

# Generate response: Y = integral(beta * X) + noise
y <- numeric(n)
for (i in 1:n) {
  y[i] <- sum(beta_true * X[i, ]) / m + rnorm(1, sd = 0.5)
}

plot(fd)
```

## Principal Component Regression

PC regression projects the functional data onto principal components and
uses the scores as predictors in a linear model.

### Basic Usage

```{r fregre-pc}
# Fit PC regression with 3 components
fit_pc <- fregre.pc(fd, y, ncomp = 3)
print(fit_pc)
```

### Examining the Fit

```{r pc-details}
# Fitted values
fitted_pc <- fit_pc$fitted.values

# Residuals
residuals_pc <- y - fitted_pc

# R-squared
r2_pc <- 1 - sum(residuals_pc^2) / sum((y - mean(y))^2)
cat("R-squared:", round(r2_pc, 3), "\n")
```

### Cross-Validation for Component Selection

```{r fregre-pc-cv}
# Find optimal number of components
cv_pc <- fregre.pc.cv(fd, y, kmax = 10)

cat("Optimal number of components:", cv_pc$ncomp.opt, "\n")
cat("CV error by component:\n")
print(round(cv_pc$cv.error, 4))
```

### Prediction

```{r predict-pc}
# Split data
train_idx <- 1:80
test_idx <- 81:100

fd_train <- fd[train_idx, ]
fd_test <- fd[test_idx, ]
y_train <- y[train_idx]
y_test <- y[test_idx]

# Fit on training data
fit_train <- fregre.pc(fd_train, y_train, ncomp = 3)

# Predict on test data
y_pred <- predict(fit_train, fd_test)

# Evaluate
cat("Test RMSE:", round(pred.RMSE(y_test, y_pred), 3), "\n")
cat("Test R2:", round(pred.R2(y_test, y_pred), 3), "\n")
```

## Basis Expansion Regression

Expands both the functional data and coefficient function in a basis
(B-spline or Fourier), then estimates coefficients with ridge regression.

### Basic Usage

```{r fregre-basis}
# Fit basis regression with 15 B-spline basis functions
fit_basis <- fregre.basis(fd, y, nbasis = 15, type = "bspline")
print(fit_basis)
```

### Regularization

The `lambda` parameter controls regularization:

```{r basis-lambda}
# Higher lambda = more regularization
fit_basis_reg <- fregre.basis(fd, y, nbasis = 15, type = "bspline", lambda = 1)
```

### Cross-Validation for Lambda

```{r fregre-basis-cv}
# Find optimal lambda
cv_basis <- fregre.basis.cv(fd, y, nbasis = 15, type = "bspline",
                            lambda = c(0, 0.001, 0.01, 0.1, 1, 10))

cat("Optimal lambda:", cv_basis$lambda.opt, "\n")
cat("CV error by lambda:\n")
print(round(cv_basis$cv.error, 4))
```

### Fourier Basis

For periodic data, use Fourier basis:

```{r basis-fourier}
fit_fourier <- fregre.basis(fd, y, nbasis = 11, type = "fourier")
```

## Nonparametric Regression

Nonparametric regression makes no assumptions about the form of the
relationship between $X$ and $Y$.

### Nadaraya-Watson Estimator

```{r fregre-np}
# Fit nonparametric regression with Nadaraya-Watson
fit_np <- fregre.np(fd, y, type.S = "S.NW")
print(fit_np)
```

### k-Nearest Neighbors

Two flavors of k-NN are available:

```{r knn}
# Global k-NN (single k for all observations)
fit_knn_global <- fregre.np(fd, y, type.S = "kNN.gCV")

# Local k-NN (adaptive k per observation)
fit_knn_local <- fregre.np(fd, y, type.S = "kNN.lCV")

cat("Global k-NN optimal k:", fit_knn_global$knn, "\n")
```

### Bandwidth Selection

```{r np-cv}
# Cross-validation for bandwidth
cv_np <- fregre.np.cv(fd, y, h.seq = seq(0.1, 1, by = 0.1))

cat("Optimal bandwidth:", cv_np$h.opt, "\n")
```

### Different Kernels

```{r kernels}
# Epanechnikov kernel
fit_epa <- fregre.np(fd, y, Ker = "epa")

# Available kernels: "norm", "epa", "tri", "quar", "cos", "unif"
```

### Different Metrics

```{r np-metrics}
# Use L1 metric instead of default L2
fit_np_l1 <- fregre.np(fd, y, metric = metric.lp, p = 1)

# Use semimetric based on PCA
fit_np_pca <- fregre.np(fd, y, metric = semimetric.pca, ncomp = 5)
```

## Comparing Methods

```{r compare}
# Fit all methods on training data
fit1 <- fregre.pc(fd_train, y_train, ncomp = 3)
fit2 <- fregre.basis(fd_train, y_train, nbasis = 15)
fit3 <- fregre.np(fd_train, y_train, type.S = "kNN.gCV")

# Predict on test data
pred1 <- predict(fit1, fd_test)
pred2 <- predict(fit2, fd_test)
pred3 <- predict(fit3, fd_test)

# Compare performance
results <- data.frame(
  Method = c("PC Regression", "Basis Regression", "k-NN"),
  RMSE = c(pred.RMSE(y_test, pred1),
           pred.RMSE(y_test, pred2),
           pred.RMSE(y_test, pred3)),
  R2 = c(pred.R2(y_test, pred1),
         pred.R2(y_test, pred2),
         pred.R2(y_test, pred3))
)
print(results)
```

## Visualizing Predictions

```{r vis-predictions}
# Create comparison data frame
df_pred <- data.frame(
  Observed = y_test,
  PC = pred1,
  Basis = pred2,
  kNN = pred3
)

# Observed vs predicted
library(ggplot2)
ggplot(df_pred, aes(x = Observed, y = PC)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "PC Regression: Observed vs Predicted",
       x = "Observed", y = "Predicted") +
  theme_minimal()
```

## Method Selection Guide

| Method | Assumptions | Strengths | Weaknesses |
|--------|-------------|-----------|------------|
| PC Regression | Linear relationship | Interpretable, fast | May miss nonlinear patterns |
| Basis Regression | Smooth coefficient | Flexible, regularized | Requires basis choice |
| k-NN | None | No assumptions | Slower, requires metric choice |

## Prediction Metrics

```{r metrics}
# Available metrics for model evaluation
cat("MAE:", pred.MAE(y_test, pred1), "\n")
cat("MSE:", pred.MSE(y_test, pred1), "\n")
cat("RMSE:", pred.RMSE(y_test, pred1), "\n")
cat("R2:", pred.R2(y_test, pred1), "\n")
```

## References

- Ramsay, J.O. and Silverman, B.W. (2005). *Functional Data Analysis*.
  Springer.
- Ferraty, F. and Vieu, P. (2006). *Nonparametric Functional Data Analysis*.
  Springer.
- Reiss, P.T. and Ogden, R.T. (2007). Functional Principal Component
  Regression and Functional Partial Least Squares. *Journal of the American
  Statistical Association*, 102(479), 984-996.
