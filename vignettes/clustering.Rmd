---
title: "Functional Clustering"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Functional Clustering}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Introduction

Functional clustering groups curves into clusters based on their similarity.
**fdars** provides k-means clustering for functional data with:

- Multiple distance metrics
- k-means++ initialization
- Automatic optimal k selection

```{r setup}
library(fdars)

# Generate data with 3 distinct clusters
set.seed(42)
n_per_cluster <- 20
m <- 100
t_grid <- seq(0, 1, length.out = m)

# Cluster 1: Sine curves
X1 <- matrix(0, n_per_cluster, m)
for (i in 1:n_per_cluster) {
  X1[i, ] <- sin(2 * pi * t_grid) + rnorm(m, sd = 0.15)
}

# Cluster 2: Cosine curves
X2 <- matrix(0, n_per_cluster, m)
for (i in 1:n_per_cluster) {
  X2[i, ] <- cos(2 * pi * t_grid) + rnorm(m, sd = 0.15)
}

# Cluster 3: Linear curves
X3 <- matrix(0, n_per_cluster, m)
for (i in 1:n_per_cluster) {
  X3[i, ] <- 2 * t_grid - 1 + rnorm(m, sd = 0.15)
}

X <- rbind(X1, X2, X3)
true_clusters <- rep(1:3, each = n_per_cluster)

fd <- fdata(X, argvals = t_grid)
plot(fd)
```

## K-Means Clustering

### Basic Usage

```{r kmeans-basic}
# Cluster into 3 groups
km <- kmeans.fd(fd, ncl = 3, seed = 123)
print(km)
```

### Visualizing Results

```{r kmeans-plot}
plot(km)
```

### Examining Cluster Assignments

```{r cluster-assignments}
# Compare to true clusters
table(Predicted = km$cluster, True = true_clusters)

# Cluster sizes
km$size

# Within-cluster sum of squares
km$withinss
```

### Multiple Random Starts

Use `nstart` to run k-means multiple times and keep the best result:

```{r nstart}
# 20 random starts
km_multi <- kmeans.fd(fd, ncl = 3, nstart = 20, seed = 123)
cat("Total within-cluster SS:", km_multi$tot.withinss, "\n")
```

## Different Distance Metrics

### String Metrics (Fast Rust Path)

For maximum speed, use string metrics that run entirely in Rust:

```{r string-metrics}
# L2 (Euclidean) - default
km_l2 <- kmeans.fd(fd, ncl = 3, metric = "L2", seed = 123)

# L1 (Manhattan)
km_l1 <- kmeans.fd(fd, ncl = 3, metric = "L1", seed = 123)

# L-infinity
km_linf <- kmeans.fd(fd, ncl = 3, metric = "Linf", seed = 123)

cat("Total WSS - L2:", km_l2$tot.withinss, "\n")
cat("Total WSS - L1:", km_l1$tot.withinss, "\n")
cat("Total WSS - Linf:", km_linf$tot.withinss, "\n")
```

### Custom Metric Functions

For flexibility, pass a metric function:

```{r custom-metrics}
# Dynamic Time Warping
km_dtw <- kmeans.fd(fd, ncl = 3, metric = metric.DTW, seed = 123)

# Hausdorff distance
km_haus <- kmeans.fd(fd, ncl = 3, metric = metric.hausdorff, seed = 123)

# PCA-based semimetric
km_pca <- kmeans.fd(fd, ncl = 3, metric = semimetric.pca, ncomp = 5, seed = 123)
```

## Optimal Number of Clusters

Choosing the right number of clusters is crucial. `optim.kmeans.fd` provides
three criteria for selecting k.

### Silhouette Score

Measures how similar curves are to their own cluster vs other clusters.
Higher is better.

```{r silhouette}
opt_sil <- optim.kmeans.fd(fd, ncl.range = 2:6,
                            criterion = "silhouette", seed = 123)
print(opt_sil)
plot(opt_sil)
```

### Calinski-Harabasz Index

Ratio of between-cluster to within-cluster variance. Higher is better.

```{r calinski}
opt_ch <- optim.kmeans.fd(fd, ncl.range = 2:6,
                          criterion = "CH", seed = 123)
print(opt_ch)
plot(opt_ch)
```

### Elbow Method

Plots within-cluster SS vs k. Look for the "elbow" where adding more
clusters doesn't help much.

```{r elbow}
opt_elbow <- optim.kmeans.fd(fd, ncl.range = 2:6,
                              criterion = "elbow", seed = 123)
print(opt_elbow)
plot(opt_elbow)
```

### Using the Optimal Model

```{r optimal-model}
# Get the best model directly
best_km <- opt_sil$best.model

# All models are stored
all_models <- opt_sil$models
```

## k-Means++ Initialization

k-means++ selects initial centers to be spread out, improving convergence:

```{r kmeanspp}
# Get initial centers using k-means++
init_centers <- kmeans.center.ini(fd, ncl = 3, seed = 123)
plot(init_centers)
```

## Comparing Clustering Solutions

### Adjusted Rand Index

Compare two clusterings (requires external package):

```{r compare-clusters, eval=FALSE}
# If you have the mclust package
library(mclust)
adjustedRandIndex(km_l2$cluster, true_clusters)
```
### Confusion Matrix

```{r confusion}
# Create contingency table
conf_matrix <- table(Predicted = km$cluster, True = true_clusters)
print(conf_matrix)

# Accuracy (after optimal label matching)
# Note: Cluster labels may be permuted
max_matches <- 0
for (perm in list(c(1,2,3), c(1,3,2), c(2,1,3), c(2,3,1), c(3,1,2), c(3,2,1))) {
  matched <- sum(km$cluster == perm[true_clusters])
  max_matches <- max(max_matches, matched)
}
cat("Best matching accuracy:", max_matches / length(true_clusters), "\n")
```

## Visualizing Cluster Centers

```{r centers}
# Extract centers
centers <- km$centers

# Plot centers alone
plot(centers)
```

## Handling Overlapping Clusters

When clusters overlap, different metrics may perform differently:

```{r overlap}
# Create overlapping data
set.seed(456)
X_overlap <- matrix(0, 60, m)
for (i in 1:30) {
  X_overlap[i, ] <- sin(2 * pi * t_grid) + rnorm(m, sd = 0.3)
}
for (i in 31:60) {
  X_overlap[i, ] <- sin(2 * pi * t_grid + 0.5) + rnorm(m, sd = 0.3)
}
fd_overlap <- fdata(X_overlap, argvals = t_grid)

# L2 may struggle with phase shifts
km_overlap_l2 <- kmeans.fd(fd_overlap, ncl = 2, metric = "L2", seed = 123)

# DTW handles phase shifts better
km_overlap_dtw <- kmeans.fd(fd_overlap, ncl = 2, metric = metric.DTW, seed = 123)

cat("L2 cluster balance:", km_overlap_l2$size, "\n")
cat("DTW cluster balance:", km_overlap_dtw$size, "\n")
```

## Performance

The Rust backend provides fast clustering:

```{r performance, eval=FALSE}
# Benchmark with 500 curves
X_large <- matrix(rnorm(500 * 200), 500, 200)
fd_large <- fdata(X_large)

system.time(kmeans.fd(fd_large, ncl = 5, metric = "L2", nstart = 10))
#>    user  system elapsed
#>   0.234   0.000   0.078

system.time(kmeans.fd(fd_large, ncl = 5, metric = metric.DTW, nstart = 10))
#>    user  system elapsed
#>   4.567   0.000   1.234
```

## Best Practices

1. **Standardize data** if curves have different scales
2. **Use multiple random starts** (`nstart >= 10`)
3. **Try different k values** with `optim.kmeans.fd`
4. **Compare metrics** when clusters may have phase shifts
5. **Visualize results** to verify cluster quality

## Summary Table

| Criterion | Interpretation | When to Use |
|-----------|----------------|-------------|
| Silhouette | -1 to 1, higher better | General purpose |
| Calinski-Harabasz | Higher better | Well-separated clusters |
| Elbow | Look for bend | Visual inspection |

| Metric | Speed | Handles Phase Shifts |
|--------|-------|---------------------|
| L2 | Fastest | No |
| L1 | Fast | No |
| DTW | Slower | Yes |
| PCA | Fast | No |

## References

- Abraham, C., Cornillon, P.A., Matzner-LÃ¸ber, E., and Molinari, N. (2003).
  Unsupervised curve clustering using B-splines. *Scandinavian Journal of
  Statistics*, 30(3), 581-595.
- Jacques, J. and Preda, C. (2014). Functional data clustering: a survey.
  *Advances in Data Analysis and Classification*, 8(3), 231-255.
